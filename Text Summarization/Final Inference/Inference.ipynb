{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import spacy\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Removes non-alphabetic characters:\n",
    "def text_strip(row):\n",
    "        \n",
    "    #ORDER OF REGEX IS VERY VERY IMPORTANT!!!!!!\n",
    "        \n",
    "    row=re.sub(\"(\\\\t)\", ' ', str(row)).lower() #remove escape charecters\n",
    "    row=re.sub(\"(\\\\r)\", ' ', str(row)).lower() \n",
    "    row=re.sub(\"(\\\\n)\", ' ', str(row)).lower()\n",
    "        \n",
    "    row=re.sub(\"(__+)\", ' ', str(row)).lower()   #remove _ if it occors more than one time consecutively\n",
    "    row=re.sub(\"(--+)\", ' ', str(row)).lower()   #remove - if it occors more than one time consecutively\n",
    "    row=re.sub(\"(~~+)\", ' ', str(row)).lower()   #remove ~ if it occors more than one time consecutively\n",
    "    row=re.sub(\"(\\+\\++)\", ' ', str(row)).lower()   #remove + if it occors more than one time consecutively\n",
    "    row=re.sub(\"(\\.\\.+)\", ' ', str(row)).lower()   #remove . if it occors more than one time consecutively\n",
    "        \n",
    "    row=re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(row)).lower() #remove <>()|&©ø\"',;?~*!\n",
    "        \n",
    "    row=re.sub(\"(mailto:)\", ' ', str(row)).lower() #remove mailto:\n",
    "    row=re.sub(r\"(\\\\x9\\d)\", ' ', str(row)).lower() #remove \\x9* in text\n",
    "    row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(row)).lower() #replace INC nums to INC_NUM\n",
    "    row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(row)).lower() #replace CM# and CHG# to CM_NUM\n",
    "        \n",
    "        \n",
    "    row=re.sub(\"(\\.\\s+)\", ' ', str(row)).lower() #remove full stop at end of words(not between)\n",
    "    row=re.sub(\"(\\-\\s+)\", ' ', str(row)).lower() #remove - at end of words(not between)\n",
    "    row=re.sub(\"(\\:\\s+)\", ' ', str(row)).lower() #remove : at end of words(not between)\n",
    "        \n",
    "    row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n",
    "        \n",
    "    #Replace any url as such https://abc.xyz.net/browse/sdf-5327 ====> abc.xyz.net\n",
    "    try:\n",
    "        url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(row))\n",
    "        repl_url = url.group(3)\n",
    "        row = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)',repl_url, str(row))\n",
    "    except:\n",
    "        pass #there might be emails with no url in them\n",
    "        \n",
    "\n",
    "        \n",
    "    row = re.sub(\"(\\s+)\",' ',str(row)).lower() #remove multiple spaces\n",
    "        \n",
    "    #Should always be last\n",
    "    row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)).lower() #remove any single charecters hanging between 2 spaces\n",
    "\n",
    "        \n",
    "        \n",
    "    yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xtokenizer15.json') as f: \n",
    "\n",
    "        data = json.load(f) \n",
    "\n",
    "        x_tokenizer15 = tokenizer_from_json(data)\n",
    "        \n",
    "with open('ytokenizer15.json') as f: \n",
    "\n",
    "        data = json.load(f) \n",
    "\n",
    "        y_tokenizer15 = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('xtokenizer65.json') as f: \n",
    "\n",
    "        data = json.load(f) \n",
    "\n",
    "        x_tokenizer65 = tokenizer_from_json(data)\n",
    "        \n",
    "with open('ytokenizer65.json') as f: \n",
    "\n",
    "        data = json.load(f) \n",
    "\n",
    "        y_tokenizer65 = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index15=y_tokenizer15.index_word\n",
    "reverse_source_word_index15=x_tokenizer15.index_word\n",
    "target_word_index15=y_tokenizer15.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index65=y_tokenizer65.index_word\n",
    "reverse_source_word_index65=x_tokenizer65.index_word\n",
    "target_word_index65=y_tokenizer65.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15=load_model('model15.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model65=load_model('model65.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary1(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index15['sostok']) and i!=target_word_index15['eostok']):\n",
    "            newString=newString+reverse_target_word_index15[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2summary2(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index65['sostok']) and i!=target_word_index65['eostok']):\n",
    "            newString=newString+reverse_target_word_index65[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_len1=100\n",
    "max_summary_len1=15\n",
    "max_text_len2=1000\n",
    "max_summary_len2=65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence1(input_seq):\n",
    "    \n",
    "    enc_emb =  model15.get_layer('embedding')\n",
    "    a=enc_emb.apply(input_seq)\n",
    "    \n",
    "    encoder_lstm = model15.get_layer('lstm')\n",
    "    encoder_output1, state_h1, state_c1 = encoder_lstm.apply(a)\n",
    "    \n",
    "    encoder_lstm1 = model15.get_layer('lstm_1')\n",
    "    encoder_output2, state_h2, state_c2 = encoder_lstm1.apply(encoder_output1)\n",
    "    \n",
    "    \n",
    "    encoder_lstm2 = model15.get_layer('lstm_2')\n",
    "    encoder_outputs, state_h, state_c= encoder_lstm2.apply(encoder_output2)\n",
    "    \n",
    "    dec_emb = model15.get_layer('embedding_1')\n",
    "    decoder_lstm = model15.get_layer('lstm_3')\n",
    "    decoder_dense = model15.get_layer('time_distributed')\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index15['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        \n",
    "        b=dec_emb.apply(target_seq)\n",
    "        \n",
    "        decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm.apply(b,initial_state=[state_h, state_c])\n",
    "      \n",
    "        output_tokens = decoder_dense.apply(decoder_outputs)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index15[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len1-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        state_h, state_c = decoder_fwd_state, decoder_back_state\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "def decode_sequence2(input_seq):\n",
    "    \n",
    "    enc_emb =  model65.get_layer('embedding')\n",
    "    a=enc_emb.apply(input_seq)\n",
    "    \n",
    "    encoder_lstm = model65.get_layer('lstm')\n",
    "    encoder_output1, state_h1, state_c1 = encoder_lstm.apply(a)\n",
    "    \n",
    "    encoder_lstm1 = model65.get_layer('lstm_1')\n",
    "    encoder_output2, state_h2, state_c2 = encoder_lstm1.apply(encoder_output1)\n",
    "    \n",
    "    \n",
    "    encoder_lstm2 = model65.get_layer('lstm_2')\n",
    "    encoder_outputs, state_h, state_c= encoder_lstm2.apply(encoder_output2)\n",
    "    \n",
    "    dec_emb = model65.get_layer('embedding_1')\n",
    "    decoder_lstm = model65.get_layer('lstm_3')\n",
    "    decoder_dense = model65.get_layer('time_distributed')\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index65['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        \n",
    "        b=dec_emb.apply(target_seq)\n",
    "        \n",
    "        decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm.apply(b,initial_state=[state_h, state_c])\n",
    "      \n",
    "        output_tokens = decoder_dense.apply(decoder_outputs)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index65[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len2-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        state_h, state_c = decoder_fwd_state, decoder_back_state\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " During a concert, 18-year-old singer Billie Eilish played a video of herself stripping off to criticise body-shaming. Billie, known for covering up her body in oversized clothes, said in a voiceover, \"If I wear what's comfortable, I'm not a woman. If I shed the layers, I'm a sl*t.\" \"Though you've never seen my body, you still judge it,\" she added.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 0.0 mins\n",
      "52\n",
      "WARNING:tensorflow:From <ipython-input-11-5f10d995308a>:4: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "Predicted summary:  start singer threatens to shoot her with her body end\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " exit\n"
     ]
    }
   ],
   "source": [
    "while 1:    \n",
    "    a=input()\n",
    "    if a=='exit':\n",
    "        break\n",
    "\n",
    "    brief_cleaning1 = text_strip(a)\n",
    "\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "    t = time()\n",
    "\n",
    "    #Batch the data points into 5000 and run on all cores for faster preprocessing\n",
    "    text = [str(doc) for doc in nlp.pipe(brief_cleaning1, batch_size=5000, n_threads=-1)]\n",
    "\n",
    "    print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "    print(len(str(text).split()))\n",
    "\n",
    "    if len(str(text).split())>100 and len(str(text).split())<1000:\n",
    "        x    =   x_tokenizer65.texts_to_sequences(text)\n",
    "        x    =   pad_sequences(x,  maxlen=max_text_len2, padding='post')\n",
    "        print(\"Predicted summary:\",decode_sequence2(x[0].reshape(1,max_text_len2)))\n",
    "    elif len(str(text).split())<101:\n",
    "        x    =   x_tokenizer15.texts_to_sequences(text)\n",
    "        x    =   pad_sequences(x,  maxlen=max_text_len1, padding='post')\n",
    "        print(\"Predicted summary:\",decode_sequence1(x[0].reshape(1,max_text_len1)))\n",
    "    else:\n",
    "        print(\"Very Big Text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
